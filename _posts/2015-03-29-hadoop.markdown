---
layout: post
title:  "Doing Hadoop Take Two"
date:   2015-03-25 
categories: Big Data
---

### Installing a standalone version of Hadoop

0. For this exercize - the t1/t2.micro machine does not seem to be sufficient.
   use at least a t2.medium
   create security groups that open ports 50070 and 8088 and assign it
   to the EC2 instance
   
1. get it

{% highlight bash %}

sudo yum install wget java-1.7.0-jdk icedtea-web
wget http://mirror.olnevhost.net/pub/apache/hadoop/common/current/hadoop-2.6.0.tar.gz
tar xzf hadoop-2.6.0.tar.gz
cd hadoop-2.6.0
vi etc/hadoop/hadoop-env.sh

# set to the root of your Java installation
export JAVA_HOME=/usr

# Assuming your installation directory is /usr/local/hadoop
export HADOOP_PREFIX=/home/ec2-user/hadoop-2.6.0

# This is just a check - dont script it
bin/hadoop

mkdir input
cp etc/hadoop/*.xml input
bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar grep input output 'dfs[a-z.]+'
cat output/*

vi etc/hadoop/core-site.xml


<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
</configuration>

vi etc/hadoop/hdfs-site.xml

<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
</configuration>

# - This is just a test
ssh localhost
ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa
cat ~/.ssh/id_dsa.pub >> ~/.ssh/authorized_keys

bin/hdfs namenode -format

sbin/start-dfs.sh

tail -f /home/ec2-user/hadoop-2.6.0/logs/hadoop-ec2-user-secondarynamenode-ip-172-31-58-199.ec2.internal.out
sudo service iptables stop

wget NameNode - http://localhost:50070
vi index.html


bin/hdfs dfs -mkdir /user/ec2-user
bin/hdfs dfs -put etc/hadoop input
bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar grep input output 'dfs[a-z.]+'
bin/hdfs dfs -get output output
cat output/*

bin/hdfs dfs -cat output/*

bin/start-yarn.sh

{% endhighlight %}
